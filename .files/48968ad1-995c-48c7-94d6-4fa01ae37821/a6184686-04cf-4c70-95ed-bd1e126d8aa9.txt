The Era of 1-bit LLMs:
All Large Language Models are in 1.58 Bits
Shuming Maâˆ—Hongyu Wangâˆ—Lingxiao Ma Lei Wang Wenhui Wang
Shaohan Huang Li Dong Ruiping Wang Jilong Xue Furu Weiâ‹„
https://aka.ms/GeneralAI
Abstract
Recent research, such as BitNet [ WMD+23], is paving the way for a new era of 1-
bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,
namely BitNet b1.58 , in which every single parameter (or weight) of the LLM is
ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer
LLM with the same model size and training tokens in terms of both perplexity
and end-task performance, while being significantly more cost-effective in terms
of latency, memory, throughput, and energy consumption. More profoundly, the
1.58-bit LLM defines a new scaling law and recipe for training new generations of
LLMs that are both high-performance and cost-effective. Furthermore, it enables
a new computation paradigm and opens the door for designing specific hardware
optimized for 1-bit LLMs.
0.2961 -0.0495
0.0413 ...â€¦ -0.4765
0.2812 0.2403
-0.1808 0.1304
-0.4809 â€¦â€¦ -0.1771
-0.1741 -0.3853Transformer LLMs
16-bit Float ( FP16/BF16)
Cost
Performance
1 -1
0â€¦â€¦ 1
-1 -1
-1 1
-1â€¦â€¦ 0
0 -1BitNet b1.58 (This Work)
{-1, 0, 1}
W=Pareto Improvement
W=
0.0413 0.3397 0.2812 0.2403
-0.1808 0.1304
-0.4809 0.32440.4322 -0.1771
-0.1741 -0.38530.2961 -0.0495 -0.0924 -0.4765 ğ’™ğŸ
ğ’™ğŸ
ğ’™ğŸ
ğ’™ğŸ‘ğŸ.ğŸğŸ—ğŸ”ğŸğ’™ğŸâˆ’ğŸ.ğŸğŸ’ğŸ—ğŸ“ğ’™ğŸâˆ’ğŸ.ğŸğŸ—ğŸğŸ’ğ’™ğŸâˆ’ğŸ.ğŸ’ğŸ•ğŸ”ğŸ“ğ’™ğŸ‘
â€¦
1 -1
0 1-1 1
-1 -1
-1 0
-1 11 -1
1 0ğ’™ğŸâˆ’ğ’™ğŸâˆ’ğ’™ğŸ+ğ’™ğŸ‘
â€¦ğ’™ğŸ
ğ’™ğŸ
ğ’™ğŸ
ğ’™ğŸ‘1(.58) -bitFP16Model W Input X Output Y Y = f(W, X)
GPU
New 
Hardware 
Figure 1: 1-bit LLMs (e.g., BitNet b1.58 ) provide a Pareto solution to reduce inference cost (latency,
throughput, and energy) of LLMs while maintaining model performance. The new computation
paradigm of BitNet b1.58 calls for actions to design new hardware optimized for 1-bit LLMs.
âˆ—Equal contribution. â‹„Corresponding author. S. Ma, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, J. Xue,
F. Wei are with Microsoft Research. H. Wang and R. Wang are with University of Chinese Academy of Sciences.arXiv:2402.17764v1  [cs.CL]  27 Feb 2024