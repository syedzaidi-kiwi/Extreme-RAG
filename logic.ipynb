{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    StorageContext, \n",
    "    ServiceContext, \n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  82%|████████▏ | 9/11 [50:02<11:07, 333.59s/it]\n",
      "Generating embeddings:  82%|████████▏ | 9/11 [48:38<10:48, 324.29s/it]\n",
      "Generating embeddings:  82%|████████▏ | 9/11 [39:46<08:50, 265.12s/it]\n"
     ]
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"/Users/kiwitech/Desktop/untitled folder\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 11/11 [00:09<00:00,  1.19it/s]\n",
      "Generating embeddings: 100%|██████████| 30/30 [00:21<00:00,  1.37it/s]\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:10<00:00,  2.07it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:12<00:00,  2.12it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:11<00:00,  2.05it/s]\n",
      "Generating embeddings: 100%|██████████| 37/37 [00:18<00:00,  2.05it/s]\n",
      "Generating embeddings: 100%|██████████| 47/47 [00:23<00:00,  2.03it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:08<00:00,  2.01it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:03<00:00,  1.90it/s]\n",
      "Parsing nodes: 100%|██████████| 9/9 [01:59<00:00, 13.23s/it]\n"
     ]
    }
   ],
   "source": [
    "embed_model = GeminiEmbedding(\n",
    "    model_name=\"models/embedding-001\", api_key=GOOGLE_API_KEY\n",
    ")\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, \n",
    "    breakpoint_percentile_threshold=95, \n",
    "    embed_model=embed_model\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(model=\"mixtral-8x7b-32768\", api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/q0jh4nhd15bf0yvkct6wrd300000gn/T/ipykernel_12113/59880135.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 9/9 [00:00<00:00, 664.89it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:05<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(documents, show_progress=True, \n",
    "               service_context=service_context, node_parser=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_rerank = CohereRerank(api_key=COHERE_API_KEY, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(service_context=service_context,\n",
    "                similarity_top_k=10,\n",
    "                node_postprocessors=[cohere_rerank],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize all the documents\"\n",
    "resp = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The documents present a study on a 1-bit Large Language Model (LLM) called BitNet b1.58. The model is based on the BitNet architecture, which uses a Transformer that replaces nn.Linear with BitLinear. BitNet b1.58 has 1.58-bit weights and 8-bit activations and is trained from scratch. The model introduces a new computation paradigm that requires almost no multiplication operations for matrix multiplication, resulting in energy savings and faster computation. BitNet b1.58 has a much lower memory footprint than full-precision models, reducing the cost and time of loading weights from DRAM.\\n\\nThe study compares BitNet b1.58 with a full-precision baseline, LLaMA LLM, and shows that BitNet b1.58 can match the performance of the full precision baseline starting from a 3B size. The memory and latency cost of BitNet b1.58 is lower than LLaMA LLM, demonstrating that BitNet b1.58 is a Pareto improvement over the state-of-the-art LLM models.\\n\\nThe study further scales up the model size to 7B, 13B, and 70B and evaluates the cost. The results show that BitNet b1.58 70B is 4.1 times faster than the LLaMA LLM baseline. The memory consumption follows a similar trend, as the embedding remains full precision and its memory proportion is smaller for larger models.\\n\\nThe study estimates the arithmetic operations energy consumption of both BitNet b1.58 and LLaMA LLM and shows that BitNet b1.58 saves 71.4 times arithmetic operations energy consumption for matrix multiplication on 7nm chips. The end-to-end energy cost shows that BitNet b1.58 becomes increasingly more efficient in terms of energy consumption compared to the FP16 LLaMA LLM baseline as the model size scales.\\n\\nThe study compares the throughput of BitNet b1.58 and LLaMA LLM with 70B parameters and shows that BitNet b1.58 70B can support up to 11 times the batch size of LLaMA LLM, resulting in an 8.9 times higher throughput.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
